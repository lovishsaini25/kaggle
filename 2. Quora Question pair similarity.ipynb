{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Objective:**\nOur main goal is to compare two sets of questions, called \"question1\" and \"question2,\" in our dataset. We're doing this because we want to figure out which questions on Quora are basically repeats of ones that have already been asked. This is really helpful because it allows us to quickly provide answers to questions that have already been answered before. To do this, we have the job of deciding whether a pair of questions are duplicates or not. We'll be submitting our predictions in a binary format, and we'll be evaluated based on a metric called log loss.\n\n**And if you find my work valuable, I would greatly appreciate your support by giving me upvotes. They're like a boost of motivation for me.**\n\nNow, let's dive into the data and get started!\n\n**Primary Goals:**\n1. **Model Accuracy:** Our foremost goal is to construct a highly accurate machine learning model. This model aims to evaluate the similarity between pairs of questions effectively. By achieving high accuracy, we aim to enhance the overall user experience on Quora, ensuring that users receive meaningful and relevant question recommendations.\n\n2. **Recommendation System:** In cases where two questions are deemed similar, we intend to utilize the model to provide users with recommendations to explore similar questions. This recommendation system is designed to enhance user engagement and satisfaction on the platform.\n\n**Considerations:**\n- **Latency:** While we prioritize accuracy, latency (response time) is not a critical concern in this context. Our focus is primarily on delivering precise recommendations, and we are willing to accept slightly longer processing times to achieve this objective.\n\n- **Interpretability:** The interpretability of the model is not a primary concern. We prioritize performance and precision over the ability to easily interpret the model's decision-making processes.\n\n- **Precision:** We emphasize the importance of precision in our model. High precision ensures that the majority of the predicted similar question pairs are indeed true positives, minimizing the risk of recommending irrelevant questions to users.\n\nIn summary, our formal agenda revolves around constructing a high-accuracy, low-latency, and high-precision model to evaluate the similarity of questions on Quora. This model will serve as the foundation for a recommendation system aimed at enhancing the user experience and engagement on the platform.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print('File name:    ', os.path.join(dirname, filename), '\\tFile Size:', str(round(os.path.getsize(os.path.join(dirname, filename)) / 1000000, 2)) + 'MB')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-26T17:53:37.138529Z","iopub.execute_input":"2023-09-26T17:53:37.138996Z","iopub.status.idle":"2023-09-26T17:53:37.187665Z","shell.execute_reply.started":"2023-09-26T17:53:37.138962Z","shell.execute_reply":"2023-09-26T17:53:37.186813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T17:53:37.189678Z","iopub.execute_input":"2023-09-26T17:53:37.190330Z","iopub.status.idle":"2023-09-26T17:53:39.791458Z","shell.execute_reply.started":"2023-09-26T17:53:37.190297Z","shell.execute_reply":"2023-09-26T17:53:39.790165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have question1 and question2 in the form of text which we wish to compare and check whether both the sentences (questions) are similar or not.\n\n* We would like to build a high accuracy model in order to ensure that the costumer experience at Quora is not hampered.\n* If two questions are similar, we could recommend the user to refer to the similar questions\n* Latency is not a major concern\n* Interpretabilty is not important. Hence ,there is no need to elucidate or substantiate the rationale behind the model's prediction of a particular class label.\n* We need a high precision model here\n\n**`id:`** Index of the data\n\n**`qid(1,2):`** Question IDs of each data\n\n**`question(1,2):`** The actual data/text of the questions.\n\n**`is_duplicate:`** The class label that we have to predict (0 for non-duplicates, 1 for duplicates).","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-26T17:53:39.792849Z","iopub.execute_input":"2023-09-26T17:53:39.793177Z","iopub.status.idle":"2023-09-26T17:53:39.801977Z","shell.execute_reply.started":"2023-09-26T17:53:39.793150Z","shell.execute_reply":"2023-09-26T17:53:39.800679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have about 400k records or question pairs.","metadata":{}},{"cell_type":"code","source":"# we have about 63% of points that are dissimilar and about 37% of the points that are similar.\n# Class label is partially balanced.\n\nprint(df['is_duplicate'].value_counts())\nprint(df['is_duplicate'].value_counts(normalize=True))\n\ndf.groupby(\"is_duplicate\")['id'].count().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T17:53:39.804964Z","iopub.execute_input":"2023-09-26T17:53:39.805351Z","iopub.status.idle":"2023-09-26T17:53:40.209296Z","shell.execute_reply.started":"2023-09-26T17:53:39.805323Z","shell.execute_reply":"2023-09-26T17:53:40.208009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We have two questions in question2 and one question in question 1 as NULL\n\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T17:53:40.211083Z","iopub.execute_input":"2023-09-26T17:53:40.212543Z","iopub.status.idle":"2023-09-26T17:53:40.530706Z","shell.execute_reply.started":"2023-09-26T17:53:40.212491Z","shell.execute_reply":"2023-09-26T17:53:40.529019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question Distribution**\n\nLet's explore the frequency of question:","metadata":{}},{"cell_type":"code","source":"total_questions_id = pd.concat([df['qid1'], df['qid2']], axis = 0)\nplt.figure(figsize=(12, 5))\nplt.hist(total_questions_id.value_counts(), bins=100)\nplt.yscale('log')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\n\n# We note that a few questions are asked multiple times and many questions occur a fewer times.\n# Out of 40k questions, 37% of the questions are duplicate","metadata":{"execution":{"iopub.status.busy":"2023-09-26T17:53:40.532568Z","iopub.execute_input":"2023-09-26T17:53:40.532931Z","iopub.status.idle":"2023-09-26T17:53:42.056399Z","shell.execute_reply.started":"2023-09-26T17:53:40.532899Z","shell.execute_reply":"2023-09-26T17:53:42.055161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Word Distribution**\n\nLet's explore the distribution of the words","metadata":{}},{"cell_type":"code","source":"# Looking at the null questions - let's remove them first. Because they won't impact your aanalysis much\n# Looks like there are 3 Null rows that I will simply drop\n\nnan_rows = df[df.isnull().any(1)]\nprint(nan_rows)\n\ndf.dropna(0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T17:53:42.058235Z","iopub.execute_input":"2023-09-26T17:53:42.058560Z","iopub.status.idle":"2023-09-26T17:53:42.698300Z","shell.execute_reply.started":"2023-09-26T17:53:42.058532Z","shell.execute_reply":"2023-09-26T17:53:42.696649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate word count for each question\ndf['no_of_words_ques1'] = df['question1'].apply(lambda row: len(row.split(\" \")))\ndf['no_of_words_ques2'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n\n\n# Prepare data for line chart\nword_counts1 = df['no_of_words_ques1'].value_counts().sort_index()\nword_counts2 = df['no_of_words_ques2'].value_counts().sort_index()\n\n# Create a line chart for word count distribution\nplt.figure(figsize=(10, 5))\n\n# Line chart for question 1\nplt.subplot(1, 2, 1)\nplt.plot(word_counts1.index, word_counts1.values, color='blue')\nplt.title('Word Count Distribution for Question 1')\nplt.xlabel('Word Count')\nplt.ylabel('Frequency')\n\n# Line chart for question 2\nplt.subplot(1, 2, 2)\nplt.plot(word_counts2.index, word_counts2.values, color='green')\nplt.title('Word Count Distribution for Question 2')\nplt.xlabel('Word Count')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# It looks like word count for question 1 follow Log normal distribution.","metadata":{"execution":{"iopub.status.busy":"2023-09-26T17:53:42.699932Z","iopub.execute_input":"2023-09-26T17:53:42.700796Z","iopub.status.idle":"2023-09-26T17:53:44.752127Z","shell.execute_reply.started":"2023-09-26T17:53:42.700753Z","shell.execute_reply":"2023-09-26T17:53:44.750797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for duplicates - no dupicate rows\n\n# df.duplicated(['qid1', 'qid2', 'question1', 'question2']).any()\n\ndf.duplicated(['question1', 'question2']).any()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T17:53:44.753768Z","iopub.execute_input":"2023-09-26T17:53:44.754089Z","iopub.status.idle":"2023-09-26T17:53:45.129312Z","shell.execute_reply.started":"2023-09-26T17:53:44.754063Z","shell.execute_reply":"2023-09-26T17:53:45.127942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nSTOP_WORDS = stopwords.words(\"english\")\n\ndef word_match(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))\n    return 1.0 * len(w1 & w2)\n\ndf['word_match'] = df.apply(word_match, axis=1)\n\ndef word_share(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\ndf['word_share'] = df.apply(word_share, axis=1)\n\n\ndf['freuqency_qid1'] = df.groupby('qid1')['qid1'].transform('count') \ndf['freuqency_qid2'] = df.groupby('qid2')['qid2'].transform('count')\ndf['len_ques1'] = df['question1'].str.len() \ndf['len_ques2'] = df['question2'].str.len()\ndf['transformed_question1'] = df.question1.apply(lambda row: ' '.join([word.replace(\"?\", \"\") for word in row.lower().split()\n                                                           if word.replace(\"?\", \"\") not in STOP_WORDS]))\ndf['transformed_question2'] = df.question2.apply(lambda row: ' '.join([word.replace(\"?\", \"\") for word in row.lower().split()\n                                                           if word.replace(\"?\", \"\") not in STOP_WORDS]))\ndf['word_common_transformed_question'] = df.apply(word_match, axis=1)\ndf['word_share_transformed_question'] = df.apply(word_share, axis=1)\ndf['transformed_question_diff'] = abs(df['transformed_question1'].apply(lambda row: len(row.split(\" \"))) - df['transformed_question2'].apply(lambda row: len(row.split(\" \"))))\n\npd.options.display.max_colwidth = 100\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T18:05:33.388527Z","iopub.execute_input":"2023-09-26T18:05:33.389853Z","iopub.status.idle":"2023-09-26T18:06:47.455190Z","shell.execute_reply.started":"2023-09-26T18:05:33.389806Z","shell.execute_reply":"2023-09-26T18:06:47.453969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T17:54:12.068795Z","iopub.execute_input":"2023-09-26T17:54:12.069302Z","iopub.status.idle":"2023-09-26T17:54:59.906912Z","shell.execute_reply.started":"2023-09-26T17:54:12.069273Z","shell.execute_reply":"2023-09-26T17:54:59.905781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-09-26T18:04:02.818472Z","iopub.execute_input":"2023-09-26T18:04:02.818946Z","iopub.status.idle":"2023-09-26T18:04:03.814813Z","shell.execute_reply.started":"2023-09-26T18:04:02.818904Z","shell.execute_reply":"2023-09-26T18:04:03.813612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}