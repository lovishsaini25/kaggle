{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Objective:**\nOur primary objective is to assess the textual similarity between two sets of questions, labeled as \"**question1**\" and \"**question2**,\" in our dataset. The dataset includes various features, such as unique identifiers (**\"qid1/qid2\"**), the actual textual content of the questions (\"**question1**\" and \"**question2**\"), and a binary classification label (\"**is_duplicate**\") indicating whether the two questions are duplicates (labeled as 1) or not (labeled as 0).\n\n**Primary Goals:**\n1. **Model Accuracy:** Our foremost goal is to construct a highly accurate machine learning model. This model aims to evaluate the similarity between pairs of questions effectively. By achieving high accuracy, we aim to enhance the overall user experience on Quora, ensuring that users receive meaningful and relevant question recommendations.\n\n2. **Recommendation System:** In cases where two questions are deemed similar, we intend to utilize the model to provide users with recommendations to explore similar questions. This recommendation system is designed to enhance user engagement and satisfaction on the platform.\n\n**Considerations:**\n- **Latency:** While we prioritize accuracy, latency (response time) is not a critical concern in this context. Our focus is primarily on delivering precise recommendations, and we are willing to accept slightly longer processing times to achieve this objective.\n\n- **Interpretability:** The interpretability of the model is not a primary concern. We prioritize performance and precision over the ability to easily interpret the model's decision-making processes.\n\n- **Precision:** We emphasize the importance of precision in our model. High precision ensures that the majority of the predicted similar question pairs are indeed true positives, minimizing the risk of recommending irrelevant questions to users.\n\nIn summary, our formal agenda revolves around constructing a high-accuracy, low-latency, and high-precision model to evaluate the similarity of questions on Quora. This model will serve as the foundation for a recommendation system aimed at enhancing the user experience and engagement on the platform.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print('File name:    ', os.path.join(dirname, filename), '\\tFile Size:', str(round(os.path.getsize(os.path.join(dirname, filename)) / 1000000, 2)) + 'MB')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-24T20:27:06.977812Z","iopub.execute_input":"2023-09-24T20:27:06.978166Z","iopub.status.idle":"2023-09-24T20:27:06.993904Z","shell.execute_reply.started":"2023-09-24T20:27:06.978139Z","shell.execute_reply":"2023-09-24T20:27:06.992437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:06.995971Z","iopub.execute_input":"2023-09-24T20:27:06.996303Z","iopub.status.idle":"2023-09-24T20:27:09.233703Z","shell.execute_reply.started":"2023-09-24T20:27:06.996276Z","shell.execute_reply":"2023-09-24T20:27:09.232506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have question1 and question2 in the form of text which we wish to compare and check whether both the sentences (questions) are similar or not.\n\n* We would like to build a high accuracy model in order to ensure that the costumer experience at Quora is not hampered.\n* If two questions are similar, we could recommend the user to refer to the similar questions\n* Latency is not a major concern\n* Interpretabilty is not important.\n* We need a high precision model here\n\n**`id:`** Index of the data\n\n**`qid(1,2):`** Question IDs of each data\n\n**`question(1,2):`** The actual data/text of the questions.\n\n**`is_duplicate:`** The class label that we have to predict (0 for non-duplicates, 1 for duplicates).","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:09.235294Z","iopub.execute_input":"2023-09-24T20:27:09.235780Z","iopub.status.idle":"2023-09-24T20:27:09.243268Z","shell.execute_reply.started":"2023-09-24T20:27:09.235700Z","shell.execute_reply":"2023-09-24T20:27:09.242108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have about 400k records or question pairs.","metadata":{}},{"cell_type":"code","source":"# we have about 63% of points that are dissimilar and about 37% of the points that are similar.\n# Class label is partially balanced.\n\nprint(df['is_duplicate'].value_counts())\nprint(df['is_duplicate'].value_counts(normalize=True))\n\ndf.groupby(\"is_duplicate\")['id'].count().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:09.244965Z","iopub.execute_input":"2023-09-24T20:27:09.245954Z","iopub.status.idle":"2023-09-24T20:27:09.583684Z","shell.execute_reply.started":"2023-09-24T20:27:09.245911Z","shell.execute_reply":"2023-09-24T20:27:09.582892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We have two questions in question2 and one question in question 1 as NULL\n\nprint(df.info())","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:09.586331Z","iopub.execute_input":"2023-09-24T20:27:09.586723Z","iopub.status.idle":"2023-09-24T20:27:09.877872Z","shell.execute_reply.started":"2023-09-24T20:27:09.586665Z","shell.execute_reply":"2023-09-24T20:27:09.876831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_questions_id = pd.concat([df['qid1'], df['qid2']], axis = 0)\nplt.figure(figsize=(12, 5))\nplt.hist(total_questions_id.value_counts(), bins=100)\nplt.yscale('log')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:09.879261Z","iopub.execute_input":"2023-09-24T20:27:09.879546Z","iopub.status.idle":"2023-09-24T20:27:11.238893Z","shell.execute_reply.started":"2023-09-24T20:27:09.879522Z","shell.execute_reply":"2023-09-24T20:27:11.237760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for duplicates - no dupicate rows\n\n# df.duplicated(['qid1', 'qid2', 'question1', 'question2']).any()\n\ndf.duplicated(['question1', 'question2']).any()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:11.240367Z","iopub.execute_input":"2023-09-24T20:27:11.240663Z","iopub.status.idle":"2023-09-24T20:27:11.661451Z","shell.execute_reply.started":"2023-09-24T20:27:11.240637Z","shell.execute_reply":"2023-09-24T20:27:11.660285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking at the null questions\n# Looks like there are 3 Null rows that I will simply drop\n\nnan_rows = df[df.isnull().any(1)]\nprint(nan_rows)\n\ndf.dropna(0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:11.662665Z","iopub.execute_input":"2023-09-24T20:27:11.663081Z","iopub.status.idle":"2023-09-24T20:27:12.267783Z","shell.execute_reply.started":"2023-09-24T20:27:11.663043Z","shell.execute_reply":"2023-09-24T20:27:12.266870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"df['freuqency_qid1'] = df.groupby('qid1')['qid1'].transform('count') \ndf['freuqency_qid2'] = df.groupby('qid2')['qid2'].transform('count')\ndf['len_ques1'] = df['question1'].str.len() \ndf['len_ques2'] = df['question2'].str.len()\ndf['#_of_words_ques1'] = df['question1'].apply(lambda row: len(row.split(\" \")))\ndf['#_of_words_ques2'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n\ndef word_common_calculator(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))\n    return 1.0 * len(w1 & w2)\ndf['word_common'] = df.apply(word_common_calculator, axis=1)\n\ndef word_share_calculator(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\ndf['word_share'] = df.apply(word_share_calculator, axis=1)\n\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:12.269233Z","iopub.execute_input":"2023-09-24T20:27:12.269691Z","iopub.status.idle":"2023-09-24T20:27:39.636099Z","shell.execute_reply.started":"2023-09-24T20:27:12.269650Z","shell.execute_reply":"2023-09-24T20:27:39.635138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import seaborn as sns\n# sns.set_style('whitegrid')\n# sns.pairplot(df.iloc[:, 5:], hue = 'is_duplicate', height = 3)\n# plt.show()\n\npd.options.display.max_colwidth = 100","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:39.637818Z","iopub.execute_input":"2023-09-24T20:27:39.638667Z","iopub.status.idle":"2023-09-24T20:27:39.644245Z","shell.execute_reply.started":"2023-09-24T20:27:39.638627Z","shell.execute_reply":"2023-09-24T20:27:39.643029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['question1', 'question2', 'is_duplicate']][df['is_duplicate'] == 0]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:39.645839Z","iopub.execute_input":"2023-09-24T20:27:39.647063Z","iopub.status.idle":"2023-09-24T20:27:39.714986Z","shell.execute_reply.started":"2023-09-24T20:27:39.647006Z","shell.execute_reply":"2023-09-24T20:27:39.713779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['question1', 'question2', 'is_duplicate']][df['is_duplicate'] == 1]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:39.716774Z","iopub.execute_input":"2023-09-24T20:27:39.717331Z","iopub.status.idle":"2023-09-24T20:27:39.775211Z","shell.execute_reply.started":"2023-09-24T20:27:39.717286Z","shell.execute_reply":"2023-09-24T20:27:39.774009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nSTOP_WORDS = stopwords.words(\"english\")\ndef remove_stop_word(row):\n    # Get the non-stopwords in Questions\n    q1_words = [word.replace(\"?\", \"\") for word in row.lower().split() if word.replace(\"?\", \"\") not in STOP_WORDS]\n#     q2_words = set([word.replace(\"?\", \"\") for word in row['question2'].lower().split() if word.replace(\"?\", \"\") not in STOP_WORDS])\n    \n    return ' '.join(q1_words)\n\ndef include_stop_word(row):\n    # Get the non-stopwords in Questions\n    q1_words = [word.replace(\"?\", \"\") for word in row.lower().split() if word.replace(\"?\", \"\") in STOP_WORDS]\n#     q2_words = set([word.replace(\"?\", \"\") for word in row['question2'].lower().split() if word.replace(\"?\", \"\") not in STOP_WORDS])\n    \n    return ' '.join(q1_words)\n\ndf['transformed_question1'] = df.question1.apply(remove_stop_word)\ndf['transformed_question2'] = df.question2.apply(remove_stop_word)\ndf['stopword_question1'] = df.question1.apply(include_stop_word)\ndf['stopword_question2'] = df.question2.apply(include_stop_word)\n\ndf['word_common_transformed_question'] = df.apply(word_common_calculator, axis=1)\ndf['word_share_transformed_question'] = df.apply(word_share_calculator, axis=1)\ndf['word_common_stopword_question'] = df.apply(word_common_calculator, axis=1)\ndf['word_share_stopword_question'] = df.apply(word_share_calculator, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:27:39.777421Z","iopub.execute_input":"2023-09-24T20:27:39.778144Z","iopub.status.idle":"2023-09-24T20:29:13.153119Z","shell.execute_reply.started":"2023-09-24T20:27:39.778092Z","shell.execute_reply":"2023-09-24T20:29:13.152025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:29:13.156445Z","iopub.execute_input":"2023-09-24T20:29:13.156949Z","iopub.status.idle":"2023-09-24T20:29:13.184820Z","shell.execute_reply.started":"2023-09-24T20:29:13.156917Z","shell.execute_reply":"2023-09-24T20:29:13.183988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['transformed_question_diff'] = df['transformed_question1'].apply(len) - df['transformed_question1'].apply(len)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:29:13.186005Z","iopub.execute_input":"2023-09-24T20:29:13.186786Z","iopub.status.idle":"2023-09-24T20:29:13.637126Z","shell.execute_reply.started":"2023-09-24T20:29:13.186749Z","shell.execute_reply":"2023-09-24T20:29:13.635849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T20:29:13.638946Z","iopub.execute_input":"2023-09-24T20:29:13.639385Z","iopub.status.idle":"2023-09-24T20:29:13.666978Z","shell.execute_reply.started":"2023-09-24T20:29:13.639310Z","shell.execute_reply":"2023-09-24T20:29:13.665750Z"},"trusted":true},"execution_count":null,"outputs":[]}]}